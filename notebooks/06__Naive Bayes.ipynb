{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e217d14",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Hasta ahora nos hemos parado principalmente en comentar el procesado y la selección de parámetros dentro de lo que se denomina como el pipeline de procesado de *machine Learning*. En las siguientes unidades abordaremos un poco más aquellas técnicas más habituales, en principio desde un punto de vista más teórico para que el lector sepa que es lo que implican cada una de ellas y después con un ejemplo de uso.\n",
    "\n",
    "Para comenzar con estás unidades, se ha seleccionado la técnica de Naive Bayes, este es un algoritmo de clasificación probabilístico basado en el teorema de Bayes. Se llama \"Naive\" debido a la suposición de independencia entre las características, que a menudo no es cierta en la realidad. Sin embargo, a pesar de esta suposición simplista, Naive Bayes tiene una gran capacidad de generalización y es ampliamente utilizado en text classification y spam filtering.un algoritmo de clasificación probabilístico basado en el teorema de Bayes. Se llama \"Naive\" debido a la suposición de independencia entre las características, que a menudo no es cierta en la realidad. Sin embargo, a pesar de esta suposición simplista, Naive Bayes tiene una gran capacidad de generalización y es ampliamente utilizado en clasificación de textos y filtros de spam.\n",
    "\n",
    "A mayores cabe destacar que es una técnica que es relativament barata en cuanto a coste computacional. Como se ha apuntado previamente se basa en el teorema de Bayes, que proporciona una forma de calcular la probabilidad condicional de un evento (en este caso, la pertenencia a una clase) dado un conjunto de evidencia (en este caso, las características de un objeto). La fórmula es la siguiente:\n",
    "\n",
    "$$ P(A | B) = P(B | A) * P(A) / P(B)$$\n",
    "\n",
    "Donde, $A$ es la clase y $B$ es el conjunto de características. Así $P(A | B)$ es la probabilidad condicional de $A$ dado $B$, es decir, la probabilidad de que un objeto pertenezca a la clase $A$ dado sus características $B$. Por su parte, $P(B | A)$ es la probabilidad de observar las características $B$ dado que pertenece a la clase $A$. Finalemnte, $P(A)$ es la probabilidad a priori de pertenecer a la clase $A$ y $P(B)$ es la probabilidad marginal de observar las características $B$.\n",
    "\n",
    "En Naive Bayes, se hace la suposición de independencia entre las características, lo que significa que se asume que la probabilidad de observar un conjunto de características $B$ dado que pertenece a la clase $A$ se puede calcular como el producto de las probabilidades individuales de cada característica dado la clase $A$, esto es:\n",
    "\n",
    "$$ P(B | A) = P(x1 | A) * P(x2 | A) * ... * P(xn | A)$$\n",
    "\n",
    "Donde $x1, x2, ..., xn$ son las características individuales de $B$.\n",
    "\n",
    "Con esta suposición, la fórmula anterior se simplifica a:\n",
    "\n",
    "$$ P(A | B) = P(A) * P(x1 | A) * P(x2 | A) * ... * P(xn | A) / P(B) $$\n",
    "\n",
    "Finalmente, se elige la clase que tenga la mayor probabilidad condicional como la clase predicha para un objeto con características $B$.\n",
    "\n",
    "En resumen, Naive Bayes utiliza el teorema de Bayes y la suposición de independencia entre características para calcular la probabilidad condicional de una clase dado un conjunto de características, y luego elegir la clase con la mayor probabilidad como la clase predicha.\n",
    "\n",
    "## Como se traduce eso a scikit-learn\n",
    "`scikit-learn` cuanta con varias implementaciones de esta aproximación que principalmente difieren de cual es el modelo que se utiliza como base para el cálculo de la probabilidad. Entre los disponibles uno de los más habituales es el que se conoce como Gaussian Naive Bayes que en la librería se encuentra en la clase `GaussianNB`. En este caso en lugar de calcular probabilidades de frecuencia, como en el caso de Naive Bayes, Gaussian Naive Bayes utiliza una distribución Gaussiana para describir la probabilidad de observar un valor dado una clase.\n",
    "\n",
    "![Naive Bayes](https://upload.wikimedia.org/wikipedia/commons/4/4f/ROC_curves.svg)\n",
    "\n",
    "La distribución Gaussiana se describe por dos parámetros: la media ($\\mu$) y la desviación estándar ($\\sigma$). La media es una medida de la tendencia central de los datos, mientras que la desviación estándar es una medida de la dispersión de los datos. Con estos dos parámetros, se puede describir la probabilidad de observar un valor $x$ dado una clase $A$ como:\n",
    "\n",
    "$$P(x | A) = 1 / (\\sqrt{2 * \\pi} * \\sigma) * exp(-(x - \\mu)^2 / 2 * \\sigma^2)$$\n",
    "\n",
    "Para cada clase, se estiman los parámetros de la distribución Gaussiana a partir de los datos de entrenamiento. Luego, se utilizan estos parámetros para calcular la probabilidad condicional de una clase dado un conjunto de características. Finalmente, se elige la clase con la mayor probabilidad como la clase predicha.\n",
    "\n",
    "En resumen, Gaussian Naive Bayes es una variante de Naive Bayes que se utiliza para clasificación de problemas con características continuas, y que utiliza una distribución Gaussiana para describir la probabilidad de observar un valor dado una clase.\n",
    "\n",
    "Dentro de la librería:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd943aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el conjunto de datos Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Crear un modelo Naive Bayes Gaussiano\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones en los datos de prueba\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Evaluar la precisión del modelo\n",
    "accuracy = gnb.score(X_test, y_test)\n",
    "print(\"Precisión: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c21a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
