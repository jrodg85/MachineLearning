{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducción de dimensionalidad\n",
    "\n",
    "existen multitud de problemas que se pueden tratar de resolver utilizando algoritmos de ML. Esto quiere decir que en algunas cosas tendremos datasets simples con pocas medidas, otras cientos y otras millones de ellas. Esto es conocido como el problema de la *maldición de la dimensionalidad*.\n",
    "\n",
    "## El problema de la maldición de las dimensionalidades\n",
    "\n",
    "Uno de los principales problemas en machine learning es el que se denomina la maldición de las dimensionalidades o de las dimensiones. Este problema surge desde el momento en el que se pretende mejorar una aproximación por el mero hecho de usar más variables. Quizá se podría, pero lo más problable es que su efecto sea contraproducente. Es aquí donde se incurre en la mencionada maldición  ya que, **a medida que aumenta el número de características o dimensiones, la cantidad de datos que son necesarios\n",
    "para obtener una generalización precisa aumenta exponencialmente.** Sin embargo antes de poder acometer el problema, es necesario saber de donde viene.\n",
    "\n",
    "![Comportamiento del rendimiento cuando se aumenta el número de variables](img/Perfomance_Dimension_plot.png)\n",
    "\n",
    "\n",
    "### La dimensionalidad de los problemas\n",
    "\n",
    "Rara vez se piensa en el impacto que tendrá una determinada variable en un proceso de optimización. Tómese por ejemplo un caso en el que se dispone de 5 observaciones para una determinada variable **X**, y dichas observaciones están uniformemente distribuidas en el espacio. De esa manera cada una de las observaciones tratará de representar a $\\frac{1}{5}$ del mencionado espacio.\n",
    "\n",
    "![](img/Sampling_Examples.png)\n",
    "\n",
    "Cuando se añade una vueva variable **Y** pasando , por tanto, a un espacio bidimensional. Con el fin de mantener la misma distancia entre las muestras, la misma tasa de representatividad del espacio se debería de aumentar el número de muestras a 25. Con una tercera se necesitarían 125 muestras para explorar el espacio en las mismas condiciones, etc. \n",
    "\n",
    "Por lo tanto, el presente problema se acrecenta de manera exponencial cuantas más dimensiones tenemos.\n",
    "\n",
    "\n",
    "### La maldición\n",
    "\n",
    "¿Qué sucede en un problema real? Pues que habitualmente no se puede aumentar el número de muestras para mantener la representatividad de los puntos muestrales y la equidistancia. Es por ello que, si agregamos una nueva característica pero no se dota de puntos suficientes, el resultado sería un modelo más complejo pero cuyo con un rendimiento empobrecido.\n",
    "\n",
    "El porqué de esa afirmación puede verse claramente con el siguiente ejemplo, en el que se pretende clasificar entre las imagenes de gatos y perros. Si solo se tiene en cuenta una dimensión, los ejemplos se encuentran uniformemente distribuidos. \n",
    "\n",
    "![](img/Doom_1.png)\n",
    "\n",
    "En este caso se dispone de 10 muestras que cubren todo el espacio. Pero, si se aumenta una dimensión, esa distribución pasa a ser algo como la siguiente figura.\n",
    "\n",
    "![](img/Doom_2.png)\n",
    "\n",
    "Esta situación podría hacer pensar que una nueva dimensión haría aun más fácil el dividir el espacio. Por ejemplo, añadiendo al problema anterior una tercera dimensión se obtine algo como:\n",
    "\n",
    "![](img/Doom_3.png)\n",
    "\n",
    "Dondé el resultado es linealmente separable como se puede ver en la siguiente imagen.\n",
    "\n",
    "![](img/Doom_4.png)\n",
    "\n",
    "La conclusión errónea a la que nos puede llevar es que, cuanto más se aumenta la dimensionalidad, más sencillo será la separación en base a las características. Nótese cómo ha variado la distribución de los datos: mientras que en una dimensión se tienen 2 muestras por cada intervalo de cinco muestras antes mencionado, en el espacio trimensional apenas llega a 0.08 muestras por intervalo (10/125). Por lo tanto, es más complicado que se encuentren contra ejemplos en un mismo lado del clasificador. El problema surge cuando proyectamos esos datos a un espacio dimensional inferior como pasa cuando se aplica cualquier red de neuronas artificiales al crear un clasificador. En esa situación el resultado sería similar a la siguiente figura:\n",
    "\n",
    "![](img/Doom_5.png)\n",
    "\n",
    "Como se puede ver en la siguiente figura, el clasificador hab sido sobreentrenado y, por lo tanto, el resultado no es tan bueno ante nuevas instancias como podría ser. Por ejemplo, véase la figura siguiente en donde se ha aplicado un clasificador lineal sencillo sobre menos dimensiones\n",
    "\n",
    "![](img/Doom_6.png)\n",
    "\n",
    "\n",
    "### ¿Cómo evitar la maldición?\n",
    "\n",
    "No existe una regla fija que defina cuántas características deben usarse en un problema de regresión/clasificación. El número dependerá de la cantidad de datos de entrenamiento disponibles, la complejidad de los límites de decisión y el tipo de clasificador utilizado. \n",
    "\n",
    "Existen principalmente dos tipos de aproximaciones con el fin de reducir la dimensionalidad. Esos dos tipos son:\n",
    "* las proyecciones (*feature selection*)\n",
    "* las transformaciones (*feature extraction*)\n",
    "\n",
    "La diferencia entre una y la otra es que, mientras que las proyecciones operan sobre el propio espacio definido por el conjunto de muestras de entrada, las transformaciones tratan de modificar dicho espacio para encontrar una función de transición que permita una representación adecuada y separable de los datos.\n",
    "Algunas de las técnicas más habituales son:\n",
    "\n",
    "* *Principal Component Analysis (PCA)*\n",
    "* *Linear Discriminant Analysis (LDA)*\n",
    "* *Independent Components Analysis (ICA)*\n",
    "* *Locally linear embedding (LLE)*\n",
    "* *t-distributed Stochastic Neighbor Embedding (t-SNE)*\n",
    "* *IsoMaps*\n",
    "* *Autoencoders*\n",
    "\n",
    "Básicamente, se entiende que una aproximación de *feature selection* es aquella en la que se seleccionan aquellas variables dentro del dataset que mejor resultado ofrecen. Mientras que por *feature extraction* se entienden aquellas aproximaciones que obtienen un número reducido o mínimo de nuevas variables, calculadas a partir de las iniciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "En este apartado veremos sólo la técnica que realmente es la más popular entre las reducciones de dimensionalidad que como es la Principal Component Analisis (PCA), si bien para aquel que quiera profundizar se recomienda cuadno menos echarle un ojo tanto a Independent Component Analysis (ICA) como a Linear Discriminant Analysis (LDA) otras dos técnicas muy populares.\n",
    "\n",
    "### Principal Component Analisis (PCA)\n",
    "\n",
    "Probablemente la técnica de reducción de dimensionalidad más utilizada. Se puede usar tanto de manera individual como en combinación con otras técnicas. Se trata de un método que transforma los datos mediante una proyección sobre un conjunto de ejes ortogonales. Para realizar este cometido, el metodo busca las mejores combinaciones lineales de las variables originales, intentando maximizar la varianza a lo largo de la nueva variable. Por ejemplo, en la imagen de la derecha que se muestra un conjunto de puntos en tres dimensiones. Esas tres dimensiones al ser proyectadas  se obtienen las imagenes de la derecha que pemiten ver la variabilidad de los datos para cada uno de los ejes. La línea continua es la que mayor variabilidad presenta y, por tanto, la que se tomará como base o primera componente. Para la segunda componente, entre las restantes posibilidades se escogerá aquella que maximiza y sigue siendo perpendicular (ortogonal) a la primera dimensión seleccionada.\n",
    "\n",
    "![](img/PCA.png)\n",
    "\n",
    "Si se necesitase una tercera dimensión, PCA tendría que buscar una que fuera perpendicualar a estas. Este proceso se basa en la conocida como matriz *Single Value Decomposition (SVD)* que extrae los autovectores del espacio de muestras. Estos son ordenados de manera decreciente y se seleccionan los que mejor representan el espacio correspondiente.\n",
    "\n",
    "Para aquellos que deseen entender en detalle como funciona, en el siguiente [enlace](https://sebastianraschka.com/Articles/2014_pca_step_by_step.html) pueden encontrar una descripción de como implementar PCA paso por paso.\n",
    "\n",
    "En terminos generales si se quiere hacer uso de PCA, una buena alternativa es el uso de la implementación como la que se puede encontrar en la librería `scikit-learn`. Dicha librería cuenta con la función `PCA` que nos permite ejecutar esta técnica de reducción. Véase el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos para la prueba\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('_data_/sonar.all_data')\n",
    "\n",
    "data.head(5)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Recoger las 60 primeras mediciones y convertirlas a un Numpy\n",
    "#no tienen nombre así que accedemos según la posición\n",
    "inputs = (data.iloc[:,0:60]).to_numpy()\n",
    "#La última columna nos marca el tipo de patron que es\n",
    "outputs = (data[60]=='M').astype('int')\n",
    "print(f\"Tipos de patrones: {np.unique(outputs)}\")\n",
    "\n",
    "#Crear los conjuntos de entrenamiento y test\n",
    "train_inputs, test_inputs, train_outputs, test_outputs = train_test_split(inputs, outputs, test_size=0.1, \n",
    "                                                                          stratify=outputs)\n",
    "\n",
    "print(f\"Train Patterns{train_inputs.shape} -> {train_outputs.shape}\")\n",
    "print(f\"Test Patterns{test_inputs.shape} -> {test_outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Definir PCA en función del que se desea conservar\n",
    "\n",
    "pca = PCA(2)\n",
    "\n",
    "#Ajustar las matrices en función de las entradas del entrenamiento\n",
    "pca.fit(train_inputs)\n",
    "\n",
    "#Una vez se tiene la transformación, simplemente es necesario aplicar la \n",
    "#transformación a los conjuntos de datos\n",
    "\n",
    "pca_train_inputs = pca.transform(train_inputs)\n",
    "pca_test_inputs = pca.transform(test_inputs)\n",
    "\n",
    "print(f\"Train Patterns{train_inputs.shape} -> {pca_train_inputs.shape}\")\n",
    "print(f\"Test Patterns{test_inputs.shape} -> {pca_test_inputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nótese que es importante que el ajuste de la transformación se haga sobre los datos de entrenamiento solamente. En caso de hacerlos sobre el total de los datos, se estaría contaminando con la transformación los posibles entrenamientos de técnicas de clasificación o regresión que pudieran aplicarse posteriormente.\n",
    "\n",
    "Una de las principales ventajas de aplicar la reducción de la dimensionalidad es que permite realizar un primer estudio visual transformando un espacio multidimensional en otro de 2 o 3 dimensiones que sí se puede representar graficamente. En el ejemplo, se utilizarán los datos trasformados por PCA para la representación. En primer lugar, se define una función para facilitar la presentación los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "def draw_results(x, colors,target_names=None):\n",
    "    \"\"\" \n",
    "        Función de utilidad que permite imprimir un scatter plot \n",
    "        con el fin de ver como reparten los clusters\n",
    "    \"\"\"\n",
    "    import matplotlib.patheffects as PathEffects\n",
    "    \n",
    "    # Seleccionar los colores (que se corresponden \n",
    "    # con el vector de salida) en función \n",
    "    # del número de clases. Este será el vector de salida.\n",
    "    num_classes = len(np.unique(colors))\n",
    "    palette = np.array(sns.color_palette(\"hls\", num_classes))\n",
    "    \n",
    "    if target_names is not None:\n",
    "        assert num_classes == len(target_names)\n",
    "        label = target_names\n",
    "    else:\n",
    "        label = [str(i) for i in range(num_classes)]\n",
    "\n",
    "    # Crear el scatter plot \n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    #Coger solo las dos primeras dimensiones de cada p\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, \n",
    "                    c=palette[colors.astype(np.int)], alpha=.8)\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    #ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # Añadir las etiquetas al listado de elementos graficos\n",
    "    txts = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        # Colocar las etiquetas en los valores medios medio del cluster\n",
    "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, label[i], fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "\n",
    "    return f, ax, sc, txts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De no haber reducido la dimensionalidad el experto sería el responsable de escoger las dos variables a representar. Al hacer esto, se correría el riego de no representar correctamente la distribución.\n",
    "A continuación, se imprime dos de las dimensiones que prefiera y compare los resultados con el obtenido por PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the PCA dataset\n",
    "draw_results(pca_train_inputs, train_outputs, target_names=train_outputs.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducir a 2 o 3 dimensiones puede ser de ayuda cuando se procura hacer un primer análisis para, por ejemplo, determinar si un clasificador lineal puede dar buenos resultados o si se observa algún patrón en la distribución de los datos. Sin embargo, lo más normal es intentar reducir la dimensionalidad pero manteniendo la mayor variabilidad posible. Para ello, la función de `scikit-learn` permite pasar un valor entre 0 y 1, que determina el porcentaje de variabilidad que debe mantener. Un valor típico es un 0.95 ya que mantiene casi toda la información relevante eliminando gran parte del ruido que pudiera haber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuación realice dicha reducción al 95%   \n",
    "pca = PCA(0.95)\n",
    "pca.fit(train_inputs)\n",
    "pca_train_inputs = pca.transform(train_inputs)\n",
    "pca_test_inputs = pca.transform(test_inputs)\n",
    "\n",
    "# Compare los tamaños con los que teníamos antes\n",
    "print(f\"Train Patterns{train_inputs.shape} -> {pca_train_inputs.shape}\")\n",
    "print(f\"Test Patterns{test_inputs.shape} -> {pca_test_inputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mayores de poder representar la información, el reducir la dimensionalidad suele venir asociado con un aceleramiento del entrenamiento. Esto se debe a que la complejidad computacional y el esfuerzo computacional de la mayoría de algoritmos de aprendizaje está condicionado en función del número de variables. Además, también es frecuente que haya una mejoría en los modelos al eliminarse parte del ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "# A continuación veamos unas cuantas aproximaciones básicas y el tiempo que tardan\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "clfs = { 'SVM': svm.SVC(probability=True), \n",
    "         'LR': LogisticRegression(),\n",
    "         'DT': DecisionTreeClassifier(max_depth=4),\n",
    "         'NB':GaussianNB()}\n",
    "base_models = ['SVM', 'LR','DT','NB']\n",
    "\n",
    "for key in clfs.keys():\n",
    "    clfs[key].fit(train_inputs, train_outputs)\n",
    "    acc = clfs[key].score(test_inputs, test_outputs)\n",
    "    print(f\"{key}: {(acc*100):.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "clfs = { 'SVM': svm.SVC(probability=True), \n",
    "         'LR': LogisticRegression(),\n",
    "         'DT': DecisionTreeClassifier(max_depth=4),\n",
    "         'NB':GaussianNB()}\n",
    "base_models = ['SVM', 'LR','DT','NB']\n",
    "\n",
    "for key in clfs.keys():\n",
    "    clfs[key].fit(pca_train_inputs, pca_train_outputs)\n",
    "    acc = clfs[key].score(pca_test_inputs, pca_test_outputs)\n",
    "    print(f\"{key}: {(acc*100):.4f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mayores es posible que se deba destacar que este tipo de elementos también pueden introducirse dentro del *pipeline* y ejecutarlo como un todo ya que no es más que otra transformación similar a la normalización o la estandarización que ya se han visto anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "La otra gran familia de técnicas para reducir la dimensionalidad tiene que ver con la selección de las variables segun su importancia. \n",
    "\n",
    "No toda la información sea realmente útil. Ese proceso se puede apreciar en la siguiente figura en la que solo una parte de los datos son datos realmente útiles y, el resto, se traduce en ruido para cualquier tipo de aproximación basada en aprendizaje automático.\n",
    "\n",
    "![](img/Feature-Selection.png)\n",
    "\n",
    "Por lo tanto,lo que se va a intentar es reducir el ruido intrinseco en los problemas. La fuente de este ruido puede ser diverso como: a datos no relacionados, variables con información redundante o variables correlacionadas o covariadas. \n",
    "\n",
    "Este proceso a veces se debe de hacer de manera semimanual, por ejemplo, imagínese que el _dataset_ tiene registrados los minutos y segundos de la duración de un evento en dos columnas diferentes dentro de un dataset tipo `pandas`. Es evidente que se pueden fusionar ambos datos en una única columna representando los minutos como segundos. Este proceso manual, si bien en muchos casos puede suponer una gran diferencia, es inabordable de manera automática ya que implica un conocimiento experto del problema. A mayores, en la mayoría de los casos actuales en donde el número de variables se cuenta por centenares o miles en muchos casos, este procedimiento manual no es factible hacerlo en un tiempo razonable salvo para casos triviales o muy conocidos. Es por ello que las técnicas de selección automática de variables cobran una especial relevancia para acometer este problema de alta dimensionalidad.\n",
    "\n",
    "Desde un punto de vista meramente organizativo, las técnicas de selección de varibles se clasifican habitualmente atendiendo a dos criterios principales cada uno de los cuales se puede ver en la figura siguiente:\n",
    "\n",
    "![](img/feature-selection-models.png)\n",
    "\n",
    "En un primer nivel estarían los modelos no supervisados, que serían aquellos que realizan realizan un procedimiento de _clustering_ sin necesidad de que el conjunto este etiquetado. Por otro lado, estarían todos los modelos supervisados, que se corresponde con aquellos en los que la selección de variables se hace en función del rendimiento de un clasificador en el que la salida debe de ser conocida. Los métodos no supervisados, a su vez, se dividen en tres grandes posibles aproximaciones referenciadas, habitualmente, por sus terminos en inglés, siendo estos _filtered_, _wrappered_ y _embedded_. En los siguientes apartados se abordarán cada una de estas tres posibles aproximaciones y detallaran algunos de los métodos más habituales que se encuadran en cada una de ellas.\n",
    "\n",
    "## _Filtered_\n",
    "Los métodos de _filtered_ o de filtrado se utilizan en el paso de preprocesado del conjunto de datos. Es decir, son previos al uso de las variables en cualquier tipo algoritmo de aprendizaje automático. Por tanto, su comportamiento es independiente de que tipo de algoritmo vaya a ser utilizado _a posteriori_. \n",
    "\n",
    "Atendiendo al coste computacional, se trata de algorimos que se puden calificar como \"baratos\" ya que su ejecución es rápida y requieren pocos recursos en terminos generales. Su principal función, es la detección y eleminación de características duplicadas, correlacionadas y redundantes. Sin embargo, debe destacarse que, este tipo de metodos, no son capaces de detectar y eliminar el ruido correspondiente a la multicolinealidad. Es decir, en este tipo de métodos, la selección de características se evalúa individualmente, lo que  puede ayudar cuando las características están aisladas (no tienen dependencia de otras características), pero tendrá un rendimiento muy penalizado cuando se usa una combinación de características como entrada, lo que podría aumentar el rendimiento global del modelo.\n",
    "\n",
    "![](img/filter-method.png)\n",
    "\n",
    "Algunas de las técnicas más utilizadas son: \n",
    "\n",
    "* Ganancia de información (_Information Gain_). Definida como la cantidad de información proporcionada por la variable para identificar el valor objetivo. En este caso se medirá la reducción de los valores de entropía y, por consiguiente, la ganancia de información aportada por cada variable teniendo la mencionada variable objetivo como referencia. \n",
    "* Test Chi-cuadrado. El método chi-cuadrado ($\\chi^2$) se utiliza generalmente para probar la relación entre variables categóricas. Compara los valores observados de diferentes atributos del conjunto de datos con su valor esperado.\n",
    "* _Fishher's Score_. El coeficiente de Fisher selecciona cada característica de forma independiente de acuerdo con sus puntuaciones según el criterio de Fisher, lo que da lugar a un conjunto subóptimo de características. Cuanto mayor es la puntuación de Fisher, mejor es la característica seleccionada.\n",
    "* Coeficiente de correlación. El coeficiente de correlación de Pearson es una medida que cuantifica la asociación entre dos variables continuas y la dirección de la relación con sus valores que van de -1 a 1.\n",
    "* Coefiente de correlación $\\tau$ de Kendall. Al igual que en el caso anterior, es un coefiente que mede la correlación entre pares de variables con la salvedad de que se aplica a rangos. Mide la similitud entre en la ordenación de los datos que se clasifican en rangos.\n",
    "* Coefiente de Correlación de Spearman ($\\rho$). Otro método que mide la similitud en base a la correlación de series de datos aleatrios. En este método se tiene en cuenta el orden en el que se presentan los datos para el calculo de la diferencia entre ellos. Se puede usar tanto con datos categóricos como continuos.\n",
    "* Umbral de varianza. (ANOVA) Es un método en el que se eliminan todas las características cuya varianza no alcanza el umbral específico tras un análisis de la varianza. Por defecto, este método elimina las características que tienen una varianza cero. La suposición que se hace con este método es que las características de mayor varianza probablemente contengan más información y sean más explicativas.\n",
    "\n",
    "Para determinar cual de las técnicas aplicar, un punto esencial es determinar el tipo de variables de entrada y salida del problema que se está abordando. En el siguiente gráfico se puede ver la sugerencia segun estos tipos si son variables numéricas o bien categóricas. \n",
    "\n",
    "![](img/Filtered_Methods_Selection.png)\n",
    "\n",
    "Así en función del tipo de problema tendríamos:\n",
    "\n",
    "* Entrada Numérica, Salida Numérica y correlación lineal: Método de Pearson ($R^2$). \n",
    "* Entrada Numérica, Salida Numérica y correlación NO lineal: Método de Spearman\n",
    "* Entrada Numérica, Salida Categórica y correlación lineal: ANOVA (Análisis de la varianza). \n",
    "* Entrada Numérica, Salida Categórica y correlación NO lineal: Método de Kendall\n",
    "* Entrada Categórica, Salida Numérica y correlación lineal: Método de Kendall \n",
    "* Entrada Categórica, Salida Numérica y correlación NO lineal: ANOVA (Análisis de la varianza)\n",
    "* Entrada Categórica, Salida Numérica: $\\chi^2$ ó Información Mutua\n",
    "\n",
    "Como nota, comentar que los problemas con entrada categórica y salida numérica son terriblemente raros. Además en la propuesta nótese que el método de Kendall asume que las categorías son ordinales, es decir, que las podemos ordenar y numerar.\n",
    "En cuanto a la implementación en Python de estos métodos se puede hacer uso tanto de la librería `scikit learn` como, en ocasiones, de `scipy`. Específicamente a continuación se muestra un resumen de las implementaciones de estas funciones:\n",
    "\n",
    "- Pearson’s Correlation Coefficient: [sklearn.feature_selection.f_regression()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)\n",
    "- Spearman’s rank correlation: [scipy.stats.spearmanr](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html)\n",
    "- ANOVA: [sklearn.feature_selection.f_classif()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html)\n",
    "- Kendall’s tau: [scipy.stats.kendalltau](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html)\n",
    "- Chi-Squared: [sklearn.feature_selection.chi2()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)\n",
    "- Mutual Information: [sklearn.feature_selection.mutual_info_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) y [sklearn.feature_selection.mutual_info_regression()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html)\n",
    "\n",
    "Para la selección se puede hacer uso de las funciones [sklearn.feature_selection.SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) y [sklearn.feature_selection.SelectPercentil](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html).Véase a continuación un ejemplo de aplicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression, f_classif\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def plot_scores(feature_filter):\n",
    "    '''\n",
    "        Función de utilidad para pintar los scores de los diferentes filtros\n",
    "    '''\n",
    "    pyplot.bar([i for i in range(len(feature_filter.scores_))], feature_filter.scores_)\n",
    "    pyplot.show()\n",
    "\n",
    "# Ejemplo con Entrada Numérica y Salida Numérica (Regresión)\n",
    "# Generar el dataset\n",
    "X, y = make_regression(n_samples=500, n_features=100, n_informative=10)\n",
    "# Definir el filtro ( Pearson)\n",
    "Pearson_filter = SelectKBest(score_func=f_regression, k=10)\n",
    "# Aplicarlo\n",
    "X_filtered = Pearson_filter.fit_transform(X, y)\n",
    "print(f'Problema de Regresión:{X.shape} - > :{X_filtered.shape}')\n",
    "plot_scores(Pearson_filter)\n",
    "\n",
    "# Ejemplo con Entrada Numérica y Salida Categorica (Clasificación)\n",
    "# Generar el dataset\n",
    "X, y = make_classification(n_samples=100, n_features=30, n_informative=2)\n",
    "# Definir el filtro (ANOVA)\n",
    "ANOVA_filter = SelectKBest(score_func=f_classif, k=2)\n",
    "# Aplicarlo\n",
    "X_filtered = ANOVA_filter.fit_transform(X, y)\n",
    "print(f'Problema de Clasificación:{X.shape} - > :{X_filtered.shape}')\n",
    "plot_scores(ANOVA_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif\n",
    "\n",
    "#Entrada y Salida Categoricas\n",
    "# Preparar el dataset a ser usado, es decir, convertir en datos categóricos \n",
    "dataset = load_breast_cancer()\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "X = ordinal_encoder.fit_transform(dataset.data)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(dataset.target)\n",
    "\n",
    "# Proceder a definir el filtro\n",
    "chi2_filter = SelectKBest(score_func=chi2, k=10)\n",
    "# Aplicarlo\n",
    "X_filtered = chi2_filter.fit_transform(X, y)\n",
    "print(f'Problema de Clasificación (CHi2):{X.shape} - > :{X_filtered.shape}')\n",
    "plot_scores(chi2_filter)\n",
    "\n",
    "# Proceder a definir el filtro\n",
    "mutual_information_filter = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "# Aplicarlo\n",
    "X_filtered = mutual_information_filter.fit_transform(X, y)\n",
    "print(f'Problema de Clasificación (MI):{X.shape} - > :{X_filtered.shape}')\n",
    "plot_scores(mutual_information_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede ver sobre estas líneas se han aplicado diferentes aproximaciones según el tipo de problema y en las figuras asociadas podemos ver el valor alcanzado por cada uno de los filtros para cada problema. Un punto importante es que, siempre que vayan a ser utilizados para procesos de _machine learning_, es necesario definir el filtro sólo sobre los datos de entrenamiento y posteriormente aplicarselo a los datos de test.  El objetivo es  no contaminar el entrenamiento con información del test. \n",
    "\n",
    "## Wrapped\n",
    "\n",
    "Los métodos _wrapped_, también denominados algoritmos _eager_, entrenan el algoritmo utilizando un subconjunto de características de forma iterativa. Basándose en las conclusiones obtenidas en el entrenamiento previo del modelo, se añaden y/o eliminan características. El o los criterios de parada para seleccionar el mejor subconjunto suelen estar predefinidos por la persona que entrena el modelo. Por ejemplo, algunas posibilidades serían cuando el rendimiento del modelo disminuye, se ha alcanzado un número específico de características, o se han realizado un número de iteraciones. Como principal ventaja de estos métodos sobre los de filtrado se puede esgrimir el hecho de que proporcionan un conjunto más óptimo de características para el entrenamiento del modelo. Esto último tiene como resultados que los modelos exhiben habitualmente una mayor precisión que los modelos entrenados tras la aplicación de los métodos de filtrado, si bien, evidentemente, son computacionalmente más caros.\n",
    "\n",
    "![](img/wrapper-method.png)\n",
    "\n",
    "Algunas de las técnicas que se puden encuadrar en este punto serían:\n",
    "\n",
    "* Selección hacia delante (_Forward Selection_). Este método es un enfoque iterativo en el que inicialmente se comienza con un conjunto vacío de características y se sigue añadiendo una característica que mejore nuestro modelo después de cada iteración. El criterio de parada es hasta que la adición de una nueva variable no mejora el rendimiento del modelo.\n",
    "* Eliminación hacia atrás (_Backward Elimination_). Este método también es un enfoque iterativo en el que inicialmente empezamos con todas las características y después de cada iteración, eliminamos la característica menos significativa. El criterio de parada es hasta que no se observa ninguna mejora en el rendimiento del modelo después de eliminar la característica.\n",
    "* Eliminación bidireccional (_Bi-directional Elimination_). Este método utiliza simultáneamente la técnica de selección hacia delante y la de eliminación hacia atrás para llegar a una única solución.\n",
    "* Selección exhaustiva (_Exhastive Selection_). Se considera un enfoque de fuerza bruta para la evaluación de subconjuntos de características. Crea todos los subconjuntos posibles y construye un algoritmo de aprendizaje para cada subconjunto y selecciona el subconjunto cuyo rendimiento del modelo es mejor.\n",
    "* Eliminación recursiva (_Recursive Feature Elimination_, RFE).Un método de optimización _eager_, el cual selecciona las características considerando recursivamente el conjunto más pequeño de características. El estimador se entrena con un conjunto inicial de características y se obtiene su importancia mediante la aportación a la solución. A continuación, se eliminan las características menos importantes del conjunto actual de características hasta que nos quedemos con el número necesario de características.\n",
    "\n",
    "Este último es sin lugar a dudas el método más aplicado dentro de este subconjunto. Fue propuesto originalmente por Guyon para los modelos _SVM_, pero tiene aplicación con cualquier algoritmo que calcule la importancia de las variables.De hecho en  `scikit learn`_ se puede hacer uso de la función correspondiente para poder acometer su uso. En el siguiente ejemplo se puede ver un ejemplo de como usar el algoritmo RFE sobre el mismo problema que se había definido en el apartado anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "model = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "rfe = RFE(model, n_features_to_select=3, verbose=3)\n",
    "rfe = rfe.fit(dataset.data, dataset.target)\n",
    "\n",
    "print(\"Num Features: %d\" % rfe.n_features_)\n",
    "print(\"Selected Features: %s\" % rfe.support_)\n",
    "print(\"Feature Ranking: %s\" % rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, el algoritmo ordena por importancia las diferentes características que lo componen. Así mismo se puede extraer una lista de las características seleccionadas. En el código siguiente se pueden ver las características con el valor de importancia asociado al correcto diagnóstico y como imprimir el nombre asociaciado en este caso a las mencionadas variables. El valor asociado es el resultado de la normalización entre 0 y 1 de las posiciones en el raking de las variables, siendo las seleccionadas aquellas con un valor de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib import pyplot\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "model = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "rfecv = RFECV(estimator=model, step=1, cv=StratifiedKFold(n_splits=10),\n",
    "              scoring='roc_auc',n_jobs=-1)\n",
    "rfecv.fit(dataset.data, dataset.target)\n",
    "\n",
    "print(f'Número óptimo de variables: {rfecv.n_features_}')\n",
    "print(f'Número total de variables: {len(dataset.target)}')\n",
    "\n",
    "pyplot.xlabel(\"Número de variables\")\n",
    "pyplot.ylabel(\"Cross validation AUROC\")\n",
    "pyplot.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el gráfico se puede ver como el sistema mejora a medida que se van introduciendo nuevas variables hasta que la cuenta llega a 26, es decir, para este modelo concreto las últimas 4 no poseen prácticamente información.\n",
    "\n",
    "## Embedded\n",
    "\n",
    "En los métodos embebidos, el algoritmo de selección de características se integra como parte del algoritmo de aprendizaje. Por lo tanto, este tiene sus propios mecanísmos para la selección de características incorporado en su funcionamiento normal. El objetivo principal de estos métodos es dar respuesta a los inconvenientes de los métodos _filtered_ y _wrapped_, mientras mantienen sus ventajas. Comparativamente estos métodos son más rápidos que los metodos _filtered_ mientras que son más precisos. Además a diferencia que estos últimos, también tienen en cuenta una combinación de características.\n",
    "\n",
    "\n",
    "![](img/embedded.png)\n",
    "\n",
    "Algunas técnicas utilizadas son\n",
    "\n",
    "* Regularización. Más que un método es un conjunto de técnicas que busca añadir una penalización a diferentes parámetros del modelo de aprendizaje automático para evitar el sobreajuste del modelo. Entre otras técnicas, tres muy utilizadas son las conocidas como _Lasso_ (o regularización L1), _Ridge_(o Regularización L2) o _ElasticNet_ (regularización L1 y L2). La regurarización L1 se utiliza cuandos se sospecha que hay características irrelievantes, mientras que la L2 se utiliza para la detección de elementos correlados. Independientemente del tipo de penalización, esta se le aplica a los coeficientes que regulan el uso de cada una de las caracterísitcas en el modelo, y pudiendo, por tanto, reducir algunos coeficientes a cero. Este último hecho permitiría eliminar aquelles características con coeficiente cero o muy bajo ya que no tendrían influencia en el modelo. A continiación a modo de recordatorio se muestra la formulación matemática de cada una de estas regularizaciones. En las fórmulas sivuinetes asuma $N$ como el número de características, $c_i$ es cada uno de los coeficientes de dichas caracterísitcas y $\\alpha$ un parámetro de ponderación entre uno y el otro tipo de normalización.\n",
    "$$ L1 = \\frac{1}{N}*\\sum_{n=1}^N |c_i|\n",
    "\\qquad \n",
    "L2 = \\frac{1}{2N}*\\sum_{n=1}^N c_i ^2\n",
    "\\qquad\n",
    "ElasticNet = \\alpha*L1 + (1-\\alpha)*L2$$\n",
    "* Métodos basados en el árboles. Otro conjunto de técnicas como pueden ser _Random Forest_ o _Gradient Boosting_. Estas técnicas utilizan un valor que ofrecen un valor de la importancia de las características que nos servirá para la selección. Por tanto, dicho valor nos indica qué características son más importantes para influir en la característica objetivo. En ambos casos, esa importancia se utiliza para la división del espacio de búsqueda por parte de los árboles.\n",
    "\n",
    "De entre las mencionadas anteriormente las de regularización son las menos usadas y a día de hoy es raro ver alguna aproximación en el sentido fuera de su aplicaión en las redes de neuronas artificiales. Por contra, el uso de los árboles de decisión y, por extensión, de los _Random Forest_ se encuentran entre los métodos más utilizados para la reducciónd e variables. Vease el siguiente ejemplo en el que se extrae la importancia que le otorga un clasificador `ExtraTrees` de la librería `scikit-learn` que crea un conjunto de arboles aleatorios sobre subconjuntos de patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier(n_estimators=10)\n",
    "model.fit(dataset.data, dataset.target)\n",
    "res = sorted(zip(map(lambda x: round(x, 4), model.feature_importances_), dataset.feature_names), reverse=False)\n",
    "(values, names) = zip(*res)\n",
    "\n",
    "pyplot.rcParams['figure.figsize'] = [10, 10]\n",
    "pyplot.barh(names, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en este gráfico no todas las caracterísitcas tienen la misma consideración en cuanto a su importancia en la clasificación y este valor puede usarse para seleccionar aquellas más importantes.\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "* Los métodos de reducción de la dimensionalidad codifican las variables y transforman el entorno y por lo tanto sería necesario deshacerlos para poder explicar el resultado, aun así su captura de variabilidad es muy importante reduciendo mucho la dimensionalidad\n",
    "* Los métodos de Selección de variables, tienen diferentes grados de precisión siendo los menos precisos los de filtrado pero a su vez los más rápidos.\n",
    "* La reducción de la dimensionalidad por el camnino que sea es un punto a tener en cuenta ya que permite reducir el tiempo de computo y obtener modelos mejores.\n",
    "* Actualmente, con las aproximacions convolucionales de las Redes de Neuronas conocidas como *Deep Learning* casi se puede restringir su uso a la aproximación embeded con la regularización.\n",
    "* En el empleo de técnicas clásicas es crucial su uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
