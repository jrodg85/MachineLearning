{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Búsqueda de hiperparámetros\n",
    "\n",
    "Hasta ahora se ha visto como se puede entrenar y validar un modelo. También se ha visto la facilidad con la que se pueden probar diferentes modelos tanto de de clasificación como de regresión, se verá más adelante el tipo de problema que nos queda: clustering.\n",
    "\n",
    "Además, se ha hecho hincapié en la importancia que tiene elegir un buen modelo y asegurarse de que no existen problemas o sesgos en los datos que puedan hacer que el análisis no sea el adecuado.\n",
    "\n",
    "Finalmente, se ha visto como cada modelo permite la configuración de algunos hiperparámetros (**OJO** a los parámetros por defecto, que no los pongamos explícitamente no quiere decir que no haya unos puestos para que el código no falle y, no quiere decir de ninguna manera, que sean los mejores).\n",
    "\n",
    "Incluso, se ha visto como se podría hacer una aproximación sencilla (manual) para ver cuáles de los hiperparámetros son los mejores para un problema concreto, con un diseño experimental adecuado (cross-validation, repeticiones, preprocesado...).\n",
    "\n",
    "Obviamente esto es algo habitual al trabajar con problemas de análisis utilizando técnicas de Machine Learning y, aunque metódico y sistemático, es tedioso. Por suerte, scikit-learn ya está pensado para hacer lo más posible este trabajo.\n",
    "\n",
    "Puede ser que seleccionen los mejores modelos mediante un primer estudio del espacio de soluciones del problema o, directamente, que nos ajustemos a 4 ó 5 modelos que nos resulten conocidos y nos permitan valorar la dificultad de problema (K-nn, Naïve Bayes, SVM, RF, ANN...). No hay nada, como se ha visto que nos permita decir que un modelo es siempre mejor que otro, o que un modelo debe ganar siempre. No hay un mal modelo, hay malos datos o modelos mal aplicados, hay que probar.\n",
    "\n",
    "En todo caso, cuando se hayan seleccionado los modelos hay que ajustarlos buscando los mejores hiperparámetros posibles (sin sobre-entrenar, que para eso se están haciendo bien las cosas con un diseño del experimento robusto). Claro, la pregunta que nos viene ahora mismo es:\n",
    "\n",
    "**¿Qué valores le pongo yo a esos famosos hiperparámetros, si no tengo ni idea?**\n",
    "\n",
    "Pues para ir viendo y conociendo los algoritmos, la mejor recomendación que se puede dar es probar valores en potencia de dos o de diez para valores continuos, y en general, para número de features o estimadores, dentro del sentido común.\n",
    "\n",
    "Hay que entender que no puede ser una regla general que valga para un dataset con 10 atributos o con 100.000.\n",
    "\n",
    "Antes de comenzar, vamos a importar algunas librerías que nos serán de utilidad en el resto simplemente para mejorar la lectura y presentación de nuestros resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rich import print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "\n",
    "Una de las herramientas principales que pone scikit-learn a nuestra disposición para el ajuste fino de modelos, es la búsqueda por grid, muy fácil de entender y de implementar.\n",
    "\n",
    "Lo único que necesita saber scikit-learn es entre qué parámetros tiene que hacer su búsqueda y poco más:\n",
    "\n",
    "Por ejemplo, vamos a trabajar con uno de los algoritmos que mejores resultados dan en general para cualquier tipo de problema, **Random Forest**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "modelo_RF = RandomForestClassifier(random_state=42)\n",
    "\n",
    "print('Parameters currently in use:')\n",
    "print(modelo_RF.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos son los valores que, por defecto, un estimador/modelo Random Forest tiene configurados por defecto. Todavía no conoce el dataset con el que tiene que trabajar, así que hay algunos parámetros que no ha calculado. Es un modelo muy utilizado en general, por lo que existen multitud de artículos y trabajos buscando la optimización de sus hiperparámetros y existen normas base para empezar.\n",
    "\n",
    "PREGUNTA: ¿habéis usado alguna vez RF? ¿sabéis alguna de esas recomendaciones?\n",
    "\n",
    "es bueno consultar la ayuda de scikit-learn, recordad que es muy completa, en concreto para [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest) en problemas de clasificación nos indica que:\n",
    "\n",
    "+ n_estimators = número de árboles en el bosque\n",
    "+ max_features = máximo número de variables consideradas para hacer el splitting del nodo\n",
    "+ max_depth = máximo número de niveles en cada decision tree\n",
    "+ min_samples_split = mínimo número de datos en cada nodo necesarios antes de hacer el split\n",
    "+ min_samples_leaf = mmínimo número de datos permitidos en cada nodo hoja\n",
    "+ bootstrap = método de sammpling de los datos (con o sin reemplazamiento)\n",
    "\n",
    "revisando la documentación de la ayuda nos encontramos con lo siguiente, a mayores de los problemas que genera que cada modelo venga con unos determinados parámetros por defecto, las implementaciones pueden diferir de las originales. Generalmente son optimizaciones que evitan perder tiempo de cómputo y están pensadas para acelerar el entrenamiento con el riesgo de una pérdida mínima de rendimiento. En todo caso, en la ayuda indican lo siguiente:\n",
    "\n",
    "*In contrast to the original publication [B2001](https://link.springer.com/article/10.1023/A:1010933404324), the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.*\n",
    "\n",
    "En general, para RF, lo convencional es ajustar los hiperparámetros *n_estimators* y *max_features*.\n",
    "\n",
    "vamos pues a preparar el grid, pero hay que tomar algunas decisiones:\n",
    "\n",
    "#### PREGUNTA: \n",
    "¿qué tipo de CV vamos a seleccionar?, ¿qué medida de acierto?\n",
    "\n",
    "\n",
    "La respuesta a ambas cuestiones es dependiente del tipo de problema y del tamaño del dataset y, se verá también que del tiempo de cómputo. No se puede probar todo.\n",
    "\n",
    "El ejemplo está orientado a un problema de clasificación, sin perder generalidad, pero debemos saber que por defecto, *GridSearchCV* va a hacer la estratificación de los samples en validación (), y usará un 3cv, a partir de la versión 0.22 es un 5cv. Para este tipo de cuestiones, es terriblemente importante que se consulte la [ayuda](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) para cada método. No nos comportemos como loros que hacen o dicen las cosas sin pensar, ni saber lo que dicen o hacen.\n",
    "\n",
    "Véase una primera aproximación inicial podría ser esta: \n",
    "\n",
    "1. en primer lugar,importamos un par de librerías que nos harán falta en el resto de ejecuciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. A continuación, vamos a crear un `GridSearch` y definir los parámetros en los que queremos buscar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parametros para la busqueda dentro del Random Forest\n",
    "\n",
    "grid_parametros =[\n",
    "    {'n_estimators':[5, 10, 100, 500, 1000], 'max_features':[2,4,6,8]},\n",
    "    {'bootstrap':[False],'n_estimators':[5, 10, 100, 500, 1000], 'max_features':[2,4,6,8]},\n",
    "]\n",
    "\n",
    "# Se define la prueba que se desea ejecutar\n",
    "grid_search = GridSearchCV(\n",
    "    modelo_RF, \n",
    "    grid_parametros, \n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Finalmente cargaremos un *dataset*, en este caso el problema de cancer de mama que viene con `scikit-learn` y ejecutaremos el grid en busca de la mejor combinación de parámetros. A mayores vemos cuanto tiempo tarda en la búsqueda para eso usaremos un elemento de Jupyter que se conoce como *magic commands*. En este caso de los muchos de que disponemos se empleará el comando `%%timeit` y que haga una única repetición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "\n",
    "dataset_breast = load_breast_cancer()\n",
    "\n",
    "grid_search.fit(dataset_breast.data,dataset_breast.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En mi equipo tarda aproximadamente '00:01:50'\n",
    "\n",
    "Si tenemos un terminal abierto en Linux con htop podríamos ver el consumo de recursos de comando anterior.\n",
    "\n",
    "#### PREGUNTA: \n",
    "¿es difícil probar este código en paralelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "\n",
    "# Es necesario redefinir el Grid ya que se ha usado antes\n",
    "grid_search = GridSearchCV(modelo_RF, grid_parametros, cv=5,\n",
    "                          scoring='roc_auc',n_jobs=4,return_train_score=True)\n",
    "\n",
    "dataset_breast = load_breast_cancer()\n",
    "\n",
    "grid_search.fit(dataset_breast.data,dataset_breast.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En mi equipo tarda aproximadamente '00:00:50', la mitad del tiempo\n",
    "\n",
    "#### PREGUNTA: \n",
    "¿qué está calculando GridSearchCV?, ¿y cómo lo calcula?\n",
    "\n",
    "Lo que está haciendo es lo siguiente:\n",
    "\n",
    "+ Evalua primero las $5 \\times 4 = 20$ combinaciones de la primera línea de grid_parametros. Especificados en un [dict](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html).\n",
    "+ Después, evalúa las $5 \\times 4 = 20$ combinaciones del segundo dict (los mismos parámetros, pero sin usar bootstrap).\n",
    "\n",
    "Pero no es solamente esto, el grid está explorando $20+20=40$ combinaciones del estimador siguiendo una aproximación 5-fold, así que ha entrenado el nada despreciable número de $40\\times5=200$ modelos. Recordad que en concreto ha realizado una única repetición del experimento.\n",
    "\n",
    "Podemos consultar los mejore parámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(GridSearchCV.get_params(grid_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No lo sabíamos, pero es relevante que, como siempre, hay parámetros por defecto que nos pueden afectar. *GridSearchCV* tiene el siguiente parámetro **'refit': True**. Esto quiere decir que todos los resultados que se muestran para el mejor modelo se han calculado en una ronda extra. Una vez que termina e identifica el mejor modelo, lo que hace es entrenar un modelo con los mejores parámetros y todos los datos. Desde la versión 1.1.8 de `scikitlearn` esto ya no es un problema se ha cambiado el valor por defecto a **False**, pero cuidado con la versión que estáis utilizando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sorted(grid_search.cv_results_.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir ahora una función de utilidad que nos va apermitir mostrar los resultados, no os dejéis intimidar por el código en si, parece enrevesado pero solo estamos definiendo un conjunto de gráficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "\n",
    "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
    "    scores_mean = cv_results['mean_test_score']\n",
    "    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    scores_sd = cv_results['std_test_score']\n",
    "    scores_sd = np.array(scores_sd).reshape(len(grid_param_2),len(grid_param_1))\n",
    "    _, ax = plt.subplots(1,1)\n",
    "    for idx, val in enumerate(grid_param_2):\n",
    "        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n",
    "\n",
    "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
    "    ax.set_xlabel(name_param_1, fontsize=16)\n",
    "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
    "    ax.legend(loc=\"best\", fontsize=15)\n",
    "    ax.grid(True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora repetiremos el entrenamiento que habíamos realizado con el problema del cancer pero, en esta ocasión limitaremos el número de parámetros a explorar para poder representarlos en 2 dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_parametros =[\n",
    "    {'n_estimators':[5, 10, 100, 500, 1000], 'max_features':[2,4,6,8]},\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(modelo_RF, grid_parametros, cv=5,\n",
    "                          scoring='roc_auc',n_jobs=4,return_train_score=True)\n",
    "\n",
    "dataset_breast = load_breast_cancer()\n",
    "\n",
    "grid_search.fit(dataset_breast.data,dataset_breast.target)\n",
    "\n",
    "# Calling Method \n",
    "plot_grid_search(grid_search.cv_results_,[5, 10, 100, 500, 1000], [2,4,6,8], 'N Estimators', 'Max Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el problema de elegir el camino 'pro' con respecto a la aproximación más cuidada, mucho cuidado conn esos parámetros por defecto. Recordad el mantra:\n",
    "\n",
    "**Scikit-learn está centrado en obtener el modelo, no interpretarlo.**\n",
    "\n",
    "Otra manera de mostrar datos en una tabla de manera rápida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"%(mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente se había visto que el mejor modelo `grid_search.best_estimator` era aquel que tenía max_features = 2 y n_estimators = 500.\n",
    "\n",
    "Si nos fijamos en la tabla, esa tabla tiene un AUC de 0.992.\n",
    "\n",
    "Ese valor de AUC lo alcanzan al menos 4 modelos, sin embargo, de nuevo, scikit-learn decide indicar que el mejor modelo es uno de ellos, pero no nos informa de nada si no lo vamos a preguntar expresamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true, y_pred = dataset_breast.target, grid_search.predict(dataset_breast.data)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde la ayuda de [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)\n",
    "\n",
    "The reported averages include micro average (averaging the total true positives, false negatives and false positives), macro average (averaging the unweighted mean per label), weighted average (averaging the support-weighted mean per label) and sample average (only for multilabel classification)\n",
    "\n",
    "Finalmente, la pregunta que falta por contestar es, ¿se puede hacer esto con repeteciones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "grid_parametros =[\n",
    "    {'n_estimators':[5, 10, 100, 500, 1000], 'max_features':[2,4,6,8]},\n",
    "]\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10)\n",
    "\n",
    "grid_search = GridSearchCV(modelo_RF, grid_parametros, cv=cv,\n",
    "                          scoring='roc_auc',n_jobs=4,return_train_score=True)\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset_breast = load_breast_cancer()\n",
    "\n",
    "grid_search.fit(dataset_breast.data,dataset_breast.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en mi equipo tarda aproximadamente '00:03:30'\n",
    "\n",
    "*GridSearchCV()* es una herramienta muy potente en términos de buscar el mejor asjuste del modelo. No sólo eso, inclusive podríamos decidir si queremos que se metan más o menos varibles y que busque la mejor combinación de estas. Esto, por ejemplo, se hace introduciendo el concepto de reducción de dimensionalidad, que vemos en próximas unidades de este curso. Vemos un ejemplo de como hacer esto mismo.\n",
    "\n",
    "Debe indicarse que, por razones de tiempo de computo, se buscarán problemas simples en la medida de la posible, ya veis que se nos dispara el tiempo de cómputo.\n",
    "\n",
    "En primer lugar vamos a cambiar de modelo y ahora en lugar de con Random Forest, vamos a usar SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "modelo_SVM = svm.SVC(gamma='state',random_state=42)\n",
    "print(svm.SVC.get_params(modelo_SVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuamos con el mismo problema de clasificación de cancer de mama, ya que es un problema de ejemplo con el que podemos trabajar sin mayores contratiempos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset_breast = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Definición de un Pipeline con los dos estadios\n",
    "pipe = Pipeline([\n",
    "    ('reduce_dim', PCA()),\n",
    "    ('classify', LinearSVC())\n",
    "])\n",
    "\n",
    "N_FEATURES_OPTIONS = [1, 2, 4, 8]\n",
    "C_OPTIONS = [512]\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduce_dim': [PCA(), NMF()],\n",
    "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "        'classify__C': C_OPTIONS\n",
    "    },\n",
    "    {\n",
    "        'reduce_dim': [SelectKBest(chi2)],\n",
    "        'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "        'classify__C': C_OPTIONS\n",
    "    },\n",
    "]\n",
    "reducer_labels = ['PCA', 'NMF', 'KBest(chi2)']\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=5, n_jobs=4, param_grid=param_grid,scoring='roc_auc')\n",
    "grid.fit(dataset_breast.data, dataset_breast.target)\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n",
    "mean_scores = mean_scores.max(axis=0)\n",
    "bar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) *\n",
    "               (len(reducer_labels) + 1) + .5)\n",
    "\n",
    "plt.figure()\n",
    "COLORS = 'bgrcmyk'\n",
    "for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n",
    "    plt.bar(bar_offsets + i, reducer_scores, label=label, color=COLORS[i])\n",
    "\n",
    "plt.title(\"Pipeline para comparar 3 técnicas de FS y un tuning de SVM\")\n",
    "plt.xlabel('Número de features')\n",
    "plt.xticks(bar_offsets + len(reducer_labels) / 2, N_FEATURES_OPTIONS)\n",
    "plt.ylabel('AUC')\n",
    "plt.ylim((0, 1.4))\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una buena opción, como siempre, podría ser importar los datos directamente a un objeto *Pandas*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "resultados = pd.DataFrame(grid.cv_results_)\n",
    "print(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(resultados[['mean_test_score','std_test_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(grid.cv_results_.get('params'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_new = SelectKBest(chi2, k=10).fit(X, y)\n",
    "\n",
    "X_new.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "datasest_breast = load_breast_cancer()\n",
    "\n",
    "grid_parametros =[\n",
    "    {'n_estimators':[500, 1000], 'max_features':[2,4]},\n",
    "]\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search = GridSearchCV(modelo_RF, grid_parametros, cv=5,\n",
    "                          scoring='roc_auc',n_jobs=4,return_train_score=True)\n",
    "\n",
    "dataset_breast = load_breast_cancer()\n",
    "\n",
    "grid_search.fit(dataset_breast.data,dataset_breast.target)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "\n",
    "names = dataset_breast.feature_names\n",
    "print(sorted(zip(map(lambda x: round(x, 4), grid_search.best_estimator_.feature_importances_), names), \n",
    "             reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dataset_breast.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res=pd.DataFrame(sorted(zip(map(lambda x: round(x, 4), grid_search.best_estimator_.feature_importances_), names), \n",
    "             reverse=True),)\n",
    "res.columns = ['Importance', 'Feature']\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res.plot.bar(x='Feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized Search\n",
    "\n",
    "La aproximación *grid search* está bien si tu exploración del espacio de soluciones se mantiene en números asumibles, ya hemos visto lo rápido que empieza a subir el tiempo de cómputo en problemas muy sencillos de juguete. Pero en el caso de que el número de hiperparámetros se dispare, scikit-learn tiene una nueva función que está pensada para ello, *RandomizedSearchCV*.\n",
    "\n",
    "la mecánica de uso de la función es muy similar a *GridSearchCV*, pero no busca todas y cada una de las combinaciones, sino que hace un número de búsquedas prefijado de combinaciones *random* utilizando en cada iteración un valor para cada hiperparámetro fijado previamente.\n",
    "\n",
    "#### PREGUNTA: \n",
    "¿ventajas?\n",
    "\n",
    "+ si no conoces el modelo te puede servir para entender qué valores suelen ser los mejores para cada técnica\n",
    "+ si permites 1000 iteraciones por ejemplo, va a probar 1000 posibles valores, en nuestro caso, es un número muy alto posibles valores para un determinado hiperparámetro.\n",
    "\n",
    "en todo caso, recordad que no hay mejores parámetros para todo posible problema que existe, aunque en general, sí se pueden encontrar algunos valores que, habitualmente, funcionan bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "import time\n",
    "\n",
    "datasest_breast = load_breast_cancer()\n",
    "\n",
    "parametros =[\n",
    "    {'n_estimators':[500, 1000], \n",
    "     'max_features':randint(20,40),\n",
    "     },\n",
    "]\n",
    "\n",
    "modelo = RandomForestClassifier()\n",
    "\n",
    "search = RandomizedSearchCV(modelo, parametros, cv=5,\n",
    "                          scoring='roc_auc',n_jobs=8, n_iter=50)\n",
    "\n",
    "dataset_breast = load_breast_cancer()\n",
    "\n",
    "search.fit(dataset_breast.data,dataset_breast.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver se han hecho dos cambios respecto del anterior código con GridSearch, en primer lugar, se ha cambiado el aprámetro `max_features` por una uniforme de cualquier número entre 20 y 40 para no limitarlo a los números exactos. A mayores e ha incluido el valor `n_iter`, que será el número de veces que se consulte a las distribuciones con el fin de hacer pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
