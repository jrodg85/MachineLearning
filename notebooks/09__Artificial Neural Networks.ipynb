{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b58216",
   "metadata": {},
   "source": [
    "# Redes de Neuronas Artificiales\n",
    "\n",
    "Entre las técnicas más populares dentro de lo que es la inteligencia artificial y más concretamente el Machine Learning, destacan sobre manera las redes de neuronas artificiales (*Artificial Neural Networks*). Esta técnica tienen su origen en la inpiración en el modelo de organzación del cerebro.  En dicho modelo, la base del cómputo es un elemento secillo como es l neurona la cual recibe un conjunto de entradas en las dendritas y tras realizar u computo genera una salida en el axioma como  se pueder en la siguiente imagen:\n",
    "\n",
    "![Neurona Biológica](https://upload.wikimedia.org/wikipedia/commons/4/44/Neuron3.png)\n",
    "\n",
    "Tomando este modelo como base, en 1947 los investigadores McCullogh y Pitts propusieron una adaptación en un modelo artificial que toma una serie de entradas y poel que las diferentes neuronas recibe un conjunto de datos como entrada a lo que se les aplica una suma ponderada como la que se ve en la siguiente imagen. \n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*WRG_Re8vGVuHDYigtq2IBA.jpeg\" width=\"480\" heigth=\"50\"/>\n",
    "\n",
    "Lo que se genera de esta manera es una combinación de hiperplanos de división en el espacio de busqueda lo que da pie a determinar regiones del espacio que pudieran ser de interés. A la salida de está combinación se le aplica una función de transferencia que puede tomar diversas formas dependiendo del tipo de comportamiento que queramos para las neuronas. Así, por ejemplo, dependiendo de está función de activación podríamos tener salidas más graduales o bien de tipo escalon o como pulsos. Cunado, la red consta de una sola neurona de este tipo se suele denominar un Perceptrón. Si bien esto es interesante, el punto crucial de redes de neuronas es que pueden ser ajustadas en un procedimiento que recibe el nombre de entrenamiento.  \n",
    "\n",
    "Al comienzo, esta técnica estaba muy limitada en su uso, ya que se creía que no podían resolver problemas no linealmente separables y se consideraban poco menos que una curiosidad teórica. De hecho, Marvin Minsky recibió el premio Alan Turing por demostrar este hecho y someter a las redes de neuronas al ostracismo científico durante más de 20 años. Ese hecho, no es hasta el año 1972, que Rummelhart, Hinton y Gibson proponen la manera en como entrenar redes con multiples capas para poder afrontar problemas no lineales. La proposición de este método, conocido como entrenamiento por backpropagation basado en la regla delta, abriría un camino para la aplicación de este tipo de sistemas en un rango mucho más amplio de problemas. Esta organización jerárquica lo que permite es combinar espacios dimensionales cada vez de un orden superior, de tal manera que el resultad de combinar planos puede dar lugar a volúmenes y su combinación para posteriormente combinarlos a su vez en otros espacios de orden superior. El resultado de está organización es lo que se viene a conocer como el perceptrón multicapa y que se ha demostrado que posee las mismas capacidades de procesado que una máquina de Turins, es decir, que puede ser utilizdo para resolver cualquier problema.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Multi-Layer_Neural_Network-Vector-Blank.svg/2880px-Multi-Layer_Neural_Network-Vector-Blank.svg.png\" width=\"480\" heigth=\"50\"/>\n",
    "\n",
    "Para combinar varios elementos diferentes y sobre todo en varias capas diferentes, existen varias técnicas que toman como base la mencionada regla delta. El procedimiento en el que se basan  es el calculo del error en la salida y la posterior modifición de los pesos en función de la contribución de los mismos en la salida.  Este tipo de técnicas, conocidas habitualmente como de gradiente descendente, son la base del procedimiento de entrenamiento de las redes de neuronas artificiales.\n",
    "\n",
    "Sin entrear de lleno en la parte matemática asociada a las reglas de entrenamiento, las cuales se basan en las derivadas de las funciones, si que es importante establecer un concepto importante en estas que no va a ser otro que el de la **función de loss**. Esta función lo que va a determinar cual es el error cometido por la función y más importante, va a dirigir el proceso de corrección de nuestra red para alcanzar el error mínimo. Existen multitud de posibilidades pero una formula habitual es el **Error Cuadrático Medio** o por sus siglas en inglés **MSE**. \n",
    "\n",
    "![Mean Square Error](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/mse.jpg.jpg)\n",
    "\n",
    "Dependiendo de la función de *loss* acometeremos el problema de una u otr a forma. A continuación vamos a resolver un pequeño problema mediente la definición de una red de neuronas. \n",
    "\n",
    "Según la información extraida de esta función de los lo que haremos será modificar los pesos de las conexiones entre las neuronas de manera iterativa en lo que se conoce como **_batches_**. Estas _batches_ lo que aglutinan es el error cometido por varios patrones con el fin de determinar el punto central en el espacio de busqueda al que actualizar buscando que no memorice.\n",
    "\n",
    "Un elemento importante en las redes es que su desempeño es muy depeniente de la calidad y cantidad de los datos provisto, por regla general se suele calcular que hacen falta del orden de 6 patrones para cada parámetro o peso que sea necesario ajustar. Como esto no siempre es posible dada las limitaciones lo que se sigue es un proceso en el que los mismos patrones se pasan varias veces en lo que se conoce como iteraciones de entrenamiento. Esto implica una serie de riesgos que comentaremos posteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3340e70",
   "metadata": {},
   "source": [
    "## Usar redes de neuronas en python\n",
    "\n",
    "Desaformatunadamente, la librería que venismo usando a lo largo del curso (**_scikit-learn_**) tiene un soporte muy pobre para la definición y el uso de redes de neuronas. Sin embargo, existen multitud de librerías para construir las redes de neuronas en python inclusive podríamos definirla de manera manuel en base a operaciones manuales. La librería que vamos a usar es una de las más conocidas a día de hoy y no es otra que **_Tensorflow_**, esta librería desarrollada por Google para dar soporte al calculo tensorial tiene como utilidad el permitir la definicón de redes de neuronas de manera sencilla, llegando a dar soporte a lo que se conoce como redes de parendizaje profundo o *Deep Learning* que veremos en una unidad posterior.\n",
    "\n",
    "En este curso haremos uso de una interface que posee y que es sencilla para su uso que recibe el nombre del **_Keras_** y que nos permitirá definir las capas de nuestras redes con una pocas llamadas a la librería como veremos a continuación.\n",
    "\n",
    "El primero de los ejemplos que vamos a abordar no será otro que el mismo ejemplo que habíamos utilizado en la unidad anterior, que no la identificación de si un objeto es una mino o una roca basado en la información integrade de las bandas de frecuencia de un sonar. Este es un problema de clasificación y como ya tenemos los ficheros descargados bastará con proceder a su carga como sigue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "file_name = '_data_/sonar.all_data'\n",
    "data = pd.read_csv(file_name, delimiter=',', header=None)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b3905",
   "metadata": {},
   "source": [
    "El siguiente paso será, como siempre, preparar los datos para proceder posteriormente a su uso en el entrenamiento, validación y test de la red que vamos a crear. Un punto importante en este punto es que vamos a utilizar un esquema conocido como **__one_hot_ecoded__**, esto es que vamo a usar dos salidas en lugar de una para nuestra clasificación y la red en cuestión nos marcará cual de las dos es la salida correcta. Usaremos este tipo de codificación ya que se ha demostrado que favorece el aprendizaje de la red debido a las operaciones matriciales que tenemos que se realizan por parte de la librería y, sobretodo debido a las derivadas que se generan durante el proceso de entrenamiento. Para ello bastará con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa98018",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Mina'] = (data[60]=='M')\n",
    "data['Roca'] = (data[60]=='R')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76c2e7",
   "metadata": {},
   "source": [
    "Con esto el siguiente paso es simplemente el proceder a crear los conjuntos de entrenamiento, validación y test. Para ello utilizaremos las función de **_scikit-learn_** destinada a tal fin como en la lección anterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "inputs = (data.iloc[:,0:60]).to_numpy()\n",
    "outputs = (data[['Mina', 'Roca']]).to_numpy().astype('int')\n",
    "\n",
    "#Creación de los conjuntos de entrenamiento, valiación y test\n",
    "train_validation_inputs, test_inputs, train_validation_outputs, test_outputs = train_test_split(inputs, outputs, test_size=0.1, stratify=outputs)\n",
    "train_inputs, validation_inputs, train_outputs, validation_outputs = train_test_split(train_validation_inputs, train_validation_outputs, test_size=0.1, stratify=train_validation_outputs)\n",
    "\n",
    "print(f\"Train Patterns{train_inputs.shape} -> {train_outputs.shape}\")\n",
    "print(f\"Validation Patterns{validation_inputs.shape} -> {validation_outputs.shape}\")\n",
    "print(f\"Test Patterns{test_inputs.shape} -> {test_outputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a22fc7",
   "metadata": {},
   "source": [
    "### El conjunto de validación y el sobre entrenamiento\n",
    "Debe de remarcarse que en este caso el conjunto de entrenamiento lo estamos dividiendo en dos nuevamente con un conjunto que hemos de nominado validación. ¿Cúal es el objetivo de este nuevo conjunto? En el procedimiento de entrenamiento las redes de neuronas pueden sufrir un problema que recibe el nombre de **sobreentrenamiento**. Este ocurre cuando las redes en lugar de generalizar el conocimento que extraen de los patrones directamente memorizan este contenido. El resultado sería que tendríamos un red con un error de entrenamiento muy bajo, pero un error en test muy alto. Para detectar el punto en el que las redes dejan de extraer este conocimiento y empiezan a memorizar, lo que se hace es usar un nuevo conjunto, el conjunto de validación. Este conjunto de datos no se utiliza durante el entrenamiento para la actualización de los pesos pero nos servirá para determinar como se comportará la red sobre el conjunto de entrenamiento cuando use datos que no ha visualizado con anterioridad.\n",
    "\n",
    "### ¿Porque no usar directamenmte el conjunto de test?\n",
    "Bueno pues porque podríamos estar parando en un punto que nos da un buen valor por mero azar sin ser su desempeño real. Es por ello que lo ideal es tener un pequeño subconjunto del entrenamiento que nos sirva para esta tarea. Así lo que haremos sera monitorizar el valor de entrenamiento y validación cuando este último empiece a incrementarse durante un número determinado de ciclos, pararemos el entrenamiento y devolveremos la red correspondiente al punto en donde se comenzo a incrementar el valor. Este procedimiento recibe el nombre de **_Early Stopping_** y veremos en el ejemplo que realizaremos su funcionamiento.\n",
    "\n",
    "<img src=\"img/Early_Stopping.png\" width=\"480\" heigth=\"50\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8792c23",
   "metadata": {},
   "source": [
    "## Definiendo una red de neuronas\n",
    "\n",
    "Una vez que ya tenemos los datos, el siguiente paso será el definir la red de neuronas. Para ello usaremos la librería **_keras_**, que forma parte tendorflow ya que ofrece una aproximación más amigable. Por ello, lo primero que vamos a hacer es  para ello lo primero que tendremos que hacer es importar la librería de **tensorflow** y, en particular, el modulo de _keras_. Para ellos bastará con la siguiente línea de código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6232aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023503c",
   "metadata": {},
   "source": [
    "Una vez hemos cargado la librería, el siguiente paso será definir la red que deseemos utilizar  o definir, esto se puede hacer de dos maneras diferentes, en el ejemplo siguiente se puede ver lo que se denomina una definición en función, en la que le pasamos todas las capas que remos que se creen especificando las características de la misma. En este ejemplo vamos a definir un modelo con 3 capas, una de entrada que no hará nada más allá de recibir los datos; una oculta con 30 elementos de procesado que recibira todas las entradas y efectuará el calculo mencionado anteriormente; y finalmente, otra capa de salida que nos determinará si es una mina o una roca. \n",
    "\n",
    "De las dos útlimas capas cabe destacar que se usan dos funciones de activación particulares como son ReLu (Rectified Linear Unsigned) y en la salida una función softmax que lo que hace es que interpretya la probabilidad de que la salida sea la correcta en relación a las otras salidas en la capa. Existe multitud de funciones de activación que se usan con diferentes objetivos, si bien estos son sólo dos ejemplos, otros posibles valores hubieran sido _linear, sigmoid, tanh, ..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b5551",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "                              keras.layers.Input(shape=(60)),\n",
    "                              keras.layers.Dense(30, activation='relu'),\n",
    "                              keras.layers.Dense(2, activation='softmax')\n",
    "                            ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60756a64",
   "metadata": {},
   "source": [
    "Una vez defino el modelo, es preciso el compilarlo, entendiendo por compilar en este contexto el definir por una parte el algoritmo que va a optimizar los pesos de la red y la función que nos facilitará el calculo de como de buena es una determinada salida. El primero de los parámetros recibe  el nombre de _optimizer_ y al igual que con la función de activación existen muchas alternmativas, en esta ocasión hemos seleccionada la *Adam* que es una mejora sobre el clásico *backpropagation* mencionado al comienzo. El segundo de los parámetros recibe el nombre de *loss* y lo que nos calculo es cuanto nos hemos equivocado en la salida. En este caso particular ya que estamos usando una función _softmax_ en la salida ese calculo lo podremos alcular con una función que se denomina *categorical_crossentropy*, otras alternativas ubiese sido usar el error cuadrático medio (RMSProps) y que se pueden consultar en el manual de tensorflow.\n",
    "Finalmente, en la misma función le indicamos que queremos monitorizar el valor de accuracy de la red, que dando por tanto está llamada como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f97295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d09e83",
   "metadata": {},
   "source": [
    "Por último comprobemos la configuración que hemos definido con "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5163b54",
   "metadata": {},
   "source": [
    "Una vez hecho esto el siguiente paso será proceder con el entrenamiento. Para ello basta con llamr a la función *fit* sobre el modelo, que en este caso le hemos pasado los siguiente parámetros:\n",
    "* los datos de entrenamiento tanto las entradas como las salidas\n",
    "* *validation_data* ya que queremos controlar el valor de validación de esa manera podremos saber si la red sobreentrena\n",
    "* *batch_size* determina cada cuantos patrones se actualizan los pesos\n",
    "* *epochs* numero de veces que se va a pasar el conjunto de entrenamiento por la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d53321",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_inputs,train_outputs,validation_data=(validation_inputs, validation_outputs), batch_size=5, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c318973",
   "metadata": {},
   "source": [
    "Si analizamos brevemente los resultados veremos que el valor de accuracy en validación es cada 10 entorno a 10 punos porcentuales más bajo qe el de entrenamiento, también veremos que este ha tenido valores más elevados previamente previos a acabar el entrenamiento. Esto nos denota dos cosas, en primer lugar, lo que podemos esperar del modelo cuando evaluemos que será similar al último valor que hemos obtenido en validación y, en segundo lugar, que indudablemente el modelo está sobre entrenandose. Veamos en primer lugar el el resultdo con los datos de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_inputs, test_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beac8b3",
   "metadata": {},
   "source": [
    "Como podemos ver es casi peor, lo que sin lugar a dudas nos indica un sobr entrenamiento. Podemos intentar hacer un par de modificiciones para mejor el dessempeño. El primero pasa por un procedimiento que se denomina EarlyStopping, buscando que la red no sobre entrene parandola si llevamucho ciclos sin mejorar. Para ello el código anterior debe de ser modificado de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = keras.callbacks.EarlyStopping( monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "                              keras.layers.Input(shape=(60)),\n",
    "                              keras.layers.Dense(30, activation='relu'),\n",
    "                              keras.layers.Dense(2, activation='softmax')\n",
    "                            ])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_inputs,train_outputs,\n",
    "          validation_data=(validation_inputs, validation_outputs), \n",
    "          batch_size=5, epochs=100, \n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e342c6a",
   "metadata": {},
   "source": [
    "Como se puede ver el entrenamiento ha sido mucho más corto, heos puesto una paciencia de 3 epoach sin mejorar el valor de validación y en este caso los valore de entrenamiento y validación son mucho más próximos en valor vemos como se comporta con los datos de test y que nos de una prueba del rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291ba680",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_inputs, test_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a904fbd",
   "metadata": {},
   "source": [
    "Como se puede ver, hemo mejorado y nos acercamos al valor que queríamos que es acercarnos a esa validación. Sin enbargo el sistema sigue teniendo problemas ya que el modelo es pequeño y además sigue sobre entrenando significativamente. Otro de los elementos que podemos usar es realizar lo que se denomina una regularización de los pesos con el fin de ajustarlos lo mejor posible. Para ello, vamos a introducir unacapa que recibe el nombre de Dropout y que previenee en parte el sobre entrenmiento. Lo que hace está capa es, básicamente, conuna cierta probabilidad anular alguno de los pesos en una iteración del entrenamiento. De esta manera se consigue que la red no memorize si no que aprenda. El cambio que vamos a realizar es sumamente sencillo quedando como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0b9618",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = keras.callbacks.EarlyStopping( monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "                              keras.layers.Input(shape=(60)),\n",
    "                              keras.layers.Dense(30, activation='relu'),\n",
    "                              keras.layers.Dropout(0.3),\n",
    "                              keras.layers.Dense(2, activation='softmax')\n",
    "                            ])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_inputs,train_outputs,\n",
    "          validation_data=(validation_inputs, validation_outputs), \n",
    "          batch_size=5, epochs=100, \n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c23c92",
   "metadata": {},
   "source": [
    "Ahora si que los resultados de entrenamiento y test se han acercado, que ocurrirá con los de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9080cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_inputs, test_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a35eb",
   "metadata": {},
   "source": [
    "En este caso al ser un modelo pequeño no ha mejorado pero nos hemos asegurado de que no haya sobre entrenamiento Esto será esencial en el siguiente capítulo cuando pasemos a utilizar modelos convolucionales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d0a3e",
   "metadata": {},
   "source": [
    "## Ejercicio propuesto\n",
    "\n",
    "Ha llegado la hora de montar una red. El ejercicio será simplemente la creación una arquitectura alternativa para este mismo problema. Para ello existendiferentes alternativas como:\n",
    "\n",
    "- preprocesar lo datos para reducir el número de entredas\n",
    "- añadir una capa más para tener una segunda capa oculta y obtenr una red de dos capas ocultas que es una de las más frecuentes en al literatura\n",
    "- Modificar el número de elementos de las capa oculta\n",
    "- Cambiar la función de loss o el optimizador\n",
    "- Alterar las variables del entrenamiento:\n",
    "  * paciencia\n",
    "  * epochs\n",
    "  * batch_size\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0c38c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
